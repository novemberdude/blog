23/11/2020

1. jupyter
 - run: jupyter notebook
 *** Question:
  + Why Spark doesn't need to install on all the YARN nodes.
  + https://www.edureka.co/blog/interview-questions/top-apache-spark-interview-questions-2016/

  ? Worker node refers to any node that can run application code in a cluster.
  ? How can you minimize transfers when you working with Apache Spark
  ?


 2. crawler

 - Các cách để chọn phần tử trong một trang web:
 + CSS: thực tế, CSS Selector được chuyển đổi ngầm thành XPath.
 + XPath: mạnh mẽ hơn CSS Selector: While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: select the link that contains the text “Next Page”. --> được khuyến khích sử dụng.

 - pipleline: use to perform more complex things with the scraped items, you don't need to implement any item piple if you just want to store the scraped items.
 - json lines:
  + utf-8 encoding
  + each line is a valid json value
  + '\n' is the line seperator
  + file extension: .jsonl
 - trùng lặp khi click một link nhiều lần: By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.

 3. spiders
 spiders are classes which define how a certain site (or a group of sites) will be scraped.
  + 1. generate the initial Request with a callback function to process the response.
  + 2. the callback funtion is used to parse the response and return item objects, Request objects or an iterable of these objects. Those Request is also contain a callback.
  + 3. finally, items returned from spiders will be typically persisted to a database (pipeline, feed export)

  *** scrapy.Spider ***
 - the most simplest spider and the one from which every spider must inherit. 
 - just provide the start_requests()
 + name: without TLD (top-level domain)
 + allowed_domains
 + start_urls
 + customer_setting
 + crawler
 + settings
 + logger
 + parse(response): this is the default callback used by Scrapy to process downloaded responses, when their requests don't specify a callback.
 + log (message [,level, compnoent])
 + closed(reason): called when spider closes.
 ??? khi nào cần định nghĩa tường minh hàm start_requests()
 - Spider Arguments:
  + spider can receive arguments that modify their behaviour. 
  + spider arguments are passed through `crawl` command using the `-a` option	
- Generic spider

### Crawl Spider
 *This is the most commonly used spider for crawling regular websites, as it provides a convenient mechanism for following links by definning a set of rule*
  - New attribute:
  `*rules*`
  `*parse_start_url*(response, **kwargs)`
  - Crawling rule
s
### XMLFeedSpider
 *Designed for parsing XML feeds by iteratings through them by a cetain node name.
### CSVFeedSpider
 *This is very similar to XMLFeedSpider, except that it iterates over rows, instead of nodes.
### SitemapSpider

## Selector
 *** trick
 scrapy <no argument>
 scrapy <command> -h
 scrapy crawl mypspider
 scrapy list
 scrapy view <url>
 scrapy shell [url]
 scrapy parse <url> [option]
 scrapy startproject myproject [projectdir]
 - sharing the root directory between projects
$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot

- 2 kinds of commands:
+ global: 
+ project-only: